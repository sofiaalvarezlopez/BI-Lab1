{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Álvaro Plata, Brenda Barahona, Sofía Álvarez (201729031)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESAI\n",
    "\n",
    "# Importamos todas las librerias necesarias para el laboratorio.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()  # for plot styling\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_profiling import ProfileReport\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>Laboratorio 1: Inteligencia de Negocios</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SaludAlpes es una entidad de salud colombiana que se especializa en atender pacientes diagnosticados con diabetes.\n",
    "Entre las actividades principales de SaludAlpes se destaca el análisis de resultados de laboratorio para la detección de diabetes.\n",
    "Este servicio de diagnóstico implica el análisis manual por parte de médicos especialistas, a partir de los resultados de exámenes de laboratorio, lo cual ha generado en los últimos años, una serie de problemas asociados con los tiempos de atención a estos pacientes, causada por demoras en la confirmación del diagnóstico.\n",
    "SaludAlpes los ha contratado para agilizar y mejorar el proceso de análisis de resultados clínicos de tal manera que se reduzcan los tiempos de confirmación de diagnóstico y se agilice el inicio del tratamiento a paciente con diabetes confirmada.\n",
    "Su labor como consultor de BI es agilizar el proceso de confirmación de diagnóstico de diabetes en un paciente, utilizando datos históricos y realizando sobre ellos procesos de análisis y procesamiento, al igual que comunicando los resultados a la entidad para afianzar sus conocimientos en áreas de aprendizaje automático y al mismo tiempo generar una ventaja competitiva.\n",
    "Con el fin de lograr el objetivo para el cual fue contratado, SaludAlpes espera tener tres modelos distintos de clasificación, construidos con 3 técnicas diferentes, entre las cuales deben estar árboles de decisión y KNN (k-nearest-neighbours). De igual manera, espera que siga la metodología que ellos utilizan en la actualidad para el desarrollo de ese tipo de proyectos,\n",
    "para lo cual le sugiere realizar los siguientes pasos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Análisis exploratorio y perfilamiento de los datos.\n",
    "\n",
    "Para poder realizar un buen algoritmo de Machine Learning, lo primero que debemos hacer es analizar y perfilar los datos, con el fin de prepararlos para "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que hacemos es leer los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = pd.read_csv('202210_Laboratorio1_data_Datos_Clasificacion_2022.csv', delimiter=';', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cabe anotar que en este caso, debemos poner el atributo de baja memoria en False. Un warning decía que hay tipos de datos mixtos y, para evitarlo, debíamos hacer eso con el parámetro de la función <code>pd.read_csv()</code>.\n",
    "\n",
    "Una vez leídos los datos, podemos ver la cantidad de columnas (features) y datos (filas) que tiene nuestro conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 27)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En principio, nuestro conjunto de datos cuenta con 100.000 registros y 27 columnas. Veamos una muestra aleatoria de 5 datos del dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diabetes_012</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>...</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "      <th>Unnamed: 23</th>\n",
       "      <th>Unnamed: 24</th>\n",
       "      <th>Unnamed: 25</th>\n",
       "      <th>Unnamed: 26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3046</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9348</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60082</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34640</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42905</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Diabetes_012 HighBP HighChol CholCheck BMI Smoker Stroke  \\\n",
       "3046             0      0        0         1  20      1      0   \n",
       "9348             0      0        0         1  35      1      0   \n",
       "60082            0      0        0         1  23      0      0   \n",
       "34640            0      0        0         1  24      1      0   \n",
       "42905            2      1        0         1  27      1      0   \n",
       "\n",
       "      HeartDiseaseorAttack PhysActivity Fruits  ... DiffWalk Sex Age  \\\n",
       "3046                     0            1      1  ...        0   1  13   \n",
       "9348                     0            1      1  ...        0   0   6   \n",
       "60082                    0            1      1  ...        0   0   8   \n",
       "34640                    0            1      0  ...        0   1   2   \n",
       "42905                    0            1      0  ...        1   1  11   \n",
       "\n",
       "      Education Income Unnamed: 22 Unnamed: 23 Unnamed: 24 Unnamed: 25  \\\n",
       "3046          6      7         NaN         NaN         NaN         NaN   \n",
       "9348          6      8         NaN         NaN         NaN         NaN   \n",
       "60082         4      7         NaN         NaN         NaN         NaN   \n",
       "34640         5      5         NaN         NaN         NaN         NaN   \n",
       "42905         5      6         NaN         NaN         NaN         NaN   \n",
       "\n",
       "      Unnamed: 26  \n",
       "3046          NaN  \n",
       "9348          NaN  \n",
       "60082         NaN  \n",
       "34640         NaN  \n",
       "42905         NaN  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(datos.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En principio, podemos intuir que la variable objetivo será <code>Diabetes_012</code>, porque de acuerdo con el diccionario del negocio, es la que nos permitirá saber si un paciente tiene diabetes, prediabetes, o ninguna condición, como es del interés de SaludAlpes. Antes de seguir con el preprocesamiento, hacemos la partición train_test, con train=80% de los datos y test=20%. No tomamos conjunto de validación pues haremos validación cruzada usando GridSearch para todos los modelos. Eliminamos la variable <code>Diabetes_012</code> de X_test y X_train y las convertimos en Y_test y Y_train, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "99995    0\n",
       "99996    0\n",
       "99997    0\n",
       "99998    0\n",
       "99999    0\n",
       "Name: Diabetes_012, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = datos['Diabetes_012']\n",
    "\n",
    "X = datos.drop(['Diabetes_012'], axis=1)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(X,Y,test_size=0.2, random_state=28)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a juntar las X, Y de entrenamiento para poder eliminar más adelante los registros completos de algunos datos, como se verá más adelante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_train = X_train.assign(Diabetes_012 = Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver el tamaño de nuestro nuevo conjunto de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del conjunto de entrenamiento es: 80000\n"
     ]
    }
   ],
   "source": [
    "print('El tamaño del conjunto de entrenamiento es: {}'.format(len(datos_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** Es importante anotar que lo primero que intentamos hacer fue un _pandas profiling_. No obstante, debido a que hay diversos tipos de datos en las columnas, este arroja varios errores. Por ello, debemos realizar un análisis exploratorio y corregir estas cuestiones antes de utilizar la herramienta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que al final de esta muestra hay 5 columnas que aparecen sin nombre: Unnamed:22 hasta Unnamed:26, cuyos valores están todos en NaN. Por ello, decidimos revisar las otras columnas del dataframe y compararlas con el diccionario de datos dado para este laboratorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker', 'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income', 'Unnamed: 22', 'Unnamed: 23', 'Unnamed: 24', 'Unnamed: 25', 'Unnamed: 26', 'Diabetes_012']\n"
     ]
    }
   ],
   "source": [
    "print(list(datos_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrastando los nombres de las columnas con el diccionario, vemos que las columnas 'Unnamed: 22', 'Unnamed: 23', 'Unnamed: 24', 'Unnamed: 25' y 'Unnamed: 26' no están caracterizadas por el negocio y, por tanto, no nos dan información relevante para el modelo. Más aún, podemos ver los valores únicos de cada una de estas columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan] [nan] [nan] [nan] [nan] "
     ]
    }
   ],
   "source": [
    "# Iteramos de 22 a 26 para conocer los valores únicos de cada una de las columnas Unnamed.\n",
    "for i in range(22,27):\n",
    "    print(datos_train['Unnamed: {num}'.format(num=i)].unique(), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos entonces que estas columnas están llenas de <code>NaN</code>; i.e. Not a Number. Por tanto, no aportan nada y procedemos a eliminarlas de nuestro conjunto de datos. Para ello, hacemos todo en funciones, con el fin de poder aplicar este pre-procesamiento al test también."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_columnas_unnamed(df, col_inicial, col_final):\n",
    "    # Eliminamos las últimas columnas\n",
    "    return df.drop(['Unnamed: {num}'.format(num=i) for i in range(col_inicial, col_final+1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_train = eliminar_columnas_unnamed(datos_train, 22, 26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisamos que las columnas hayan sido correctamente eliminadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker', 'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income', 'Diabetes_012']\n"
     ]
    }
   ],
   "source": [
    "print(list(datos_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, ya tenemos todas las columnas que se encuentran en el diccionario. Lo siguiente que queremos ver es explorar los datos que hay en nuestro dataset. Nos preguntamos: ¿Por qué, al comienzo, tuvimos un _warning_ con los tipos de datos? ¿Qué datos están almacenados en el conjunto?\n",
    "\n",
    "Revisando el diccionario de datos, nos dimos cuenta de que todos los atributos debían ser numéricos (así fueran categorías numéricas - como es el caso de columnas como GenHlth, Age y Education, entre otras - o binarias). Se nos ocurrió, entonces, revisar si las columnas estaban presentando otro tipo de datos no numéricos. Para ello, primero buscamos en el conjunto de datos todos los valores distintos que hay en nuestro dataframe y los convertimos en una lista. Luego, iteramos sobre ellos e intentamos convertirlos (hacer el cast) a números flotantes, usando un bloque try-except. Si se lanza una excepción, significa que el dato no es numérico y lo agregamos a la lista, para saber qué otro tipo de datos tiene nuestro dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los valores no numéricos son: ['-', '?', 'Xx']\n"
     ]
    }
   ],
   "source": [
    "# Vemos todos los valores distintos (únicos) que tiene el dataframe.\n",
    "valores_distintos = list(datos_train.apply(pd.value_counts).index)\n",
    "valores_no_numericos = []\n",
    "# Interamos sobre los valores distintos\n",
    "for val in valores_distintos:\n",
    "    try:\n",
    "        # Intentamos hacer el cast a float\n",
    "        numero = float(val)\n",
    "    except:\n",
    "        # Si hay datos no numericos, los agregamos a la lista\n",
    "        valores_no_numericos.append(val)\n",
    "print('Los valores no numéricos son: {}'.format(valores_no_numericos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del procedimiento anterior, podemos ver que hay 3 tipos de datos no numéricos en todo el dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debemos tomar una decisión sobre si completamos estos datos con medidas de tendencia central (en este caso, como la moda, porque son atributos categóricos), o si los eliminamos. Para tomar estas decisiones, debemos saber en cuántos datos del conjunto total hay estas inconsistencias. \n",
    "\n",
    "Es importante anotar que vamos fila por fila en el conjunto de datos porque, si revisamos en columnas (por ejemplo, usando las funcionalidades de <code>pandas</code>: <code>datos[columna] == '-'</code>) puede ocurrir que, para el mismo dato en otra columna, haya otro '-', o una 'Xx' y un '?'. Esto es difícil saberlo en pandas y, por ello, recorremos todo el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número de filas con datos inconsistentes es: 589\n"
     ]
    }
   ],
   "source": [
    "num_inconsistentes = len(datos_train[((datos_train.values == '-')|(datos_train.values == '?')|(datos_train.values == 'Xx')).any(axis=1)])\n",
    "print('El número de filas con datos inconsistentes es: {}'.format(num_inconsistentes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA:**  El código de la celda de arriba hace lo mismo que el snippet de código mostrado a continuación:\n",
    "\n",
    "```python\n",
    "num_inconsistentes = 0\n",
    "for line in datos.values:\n",
    "    if ('-' in line) or ('?' in line) or ('Xx' in line):\n",
    "        # Suma uno si hay alguno de estos símbolos. \n",
    "        num_inconsistentes += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos el porcentaje de datos inválidos con respecto al tamaño del conjunto de datos. Note que en este punto consideramos inválidos todos aquellos datos que no son categóricos, aún no hemos evaluado las restricciones individuales de cada columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El porcentaje de datos inválidos es del: 0.74%\n"
     ]
    }
   ],
   "source": [
    "porcentaje_invalidos_general = num_inconsistentes*100/len(datos_train)\n",
    "print('El porcentaje de datos inválidos es del: {:.2f}%'.format(porcentaje_invalidos_general))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el porcentaje de datos con los caracteres <code>-</code>, <code>?</code> y <code>Xx</code> es bastante reducido y, por tanto, podemos eliminarlos del dataset sin ningún problema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_basura(df, basura=['-', 'Xx', '?']):\n",
    "    for b in basura:\n",
    "        df.drop(df[df.values == b].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eliminar_basura(datos_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datos.drop(datos[(datos.values == '-') | (datos.values == 'Xx') | (datos.values == '?')].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del nuevo conjunto de datos es: 79411\n"
     ]
    }
   ],
   "source": [
    "print('El tamaño del nuevo conjunto de datos es: {}'.format(len(datos_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acuerdo con el diccionario del negocio, ahora las variables deben tener únicamente valores numéricos (enteros, para las categóricas o flotantes, para la variable BMI). Revisemos los tipos de datos del dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HighBP                  object\n",
      "HighChol                object\n",
      "CholCheck               object\n",
      "BMI                     object\n",
      "Smoker                  object\n",
      "Stroke                  object\n",
      "HeartDiseaseorAttack    object\n",
      "PhysActivity            object\n",
      "Fruits                  object\n",
      "Veggies                 object\n",
      "HvyAlcoholConsump       object\n",
      "AnyHealthcare           object\n",
      "NoDocbcCost             object\n",
      "GenHlth                 object\n",
      "MentHlth                object\n",
      "PhysHlth                object\n",
      "DiffWalk                object\n",
      "Sex                     object\n",
      "Age                     object\n",
      "Education               object\n",
      "Income                  object\n",
      "Diabetes_012            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(datos_train.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que todas las columnas son de tipo object; y esto se debe, seguramente, a que hay algunos números que está tomando como tal y, otros, como strings. Revisemos, por ejemplo, la columna <code>Diabetes_012</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '2' '1' nan]\n"
     ]
    }
   ],
   "source": [
    "print(datos_train['Diabetes_012'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirmamos entonces que el conjunto de datos está tomando algunos valores numéricos como strings. Convertimos, entonces, estas columnas a datos numéricos. Todas las convertimos a enteros, excepción hecha de la columna BMI que, por el negocio, puede ser cualquier valor real. Además, nos percatamos de que hay algunos datos que son NaN (i.e. Not a Number). De ellos nos encargaremos más adelante.\n",
    "\n",
    "De esta forma, convertimos todas las columnas primero a <code>float</code>, porque no podemos convertir las  columnas a <code>int</code> sin lidiar con los valores nulos. En resumen, primero pasaremos los datos a <code>float</code>, luego nos ocuparemos de los valores NaN y finalmente sí convertiremos todo a <code>int</code> (a excepción de BMI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero convertimos todas las columnas a float\n",
    "datos_train = datos_train.astype(dtype='float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empecemos a lidiar con los valores nulos. Veamos primero, en total, cuantos valores nulos hay en el dataset y que porcentaje representan estos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay : 267 datos NaN, lo cual representa el 0.34% de datos train.\n"
     ]
    }
   ],
   "source": [
    "num_NaN=datos_train.isnull().any(axis = 1).sum()\n",
    "porcentaje= num_NaN/datos_train.shape[0] *100\n",
    "print('Hay : {}'.format(num_NaN), \"datos NaN, lo cual representa el {:.2f}%\".format(porcentaje), \"de datos train.\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque pueden haber filas con más de un valor NaN, sabemos que el máximo valor posible de filas con valores NaN es de 0.34%. Como este porcentaje es muy pequeño, procederemos a eliminar estas filas de datos train.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del nuevo conjunto de datos es: 79144\n"
     ]
    }
   ],
   "source": [
    "datos_train=datos_train.dropna()\n",
    "print('El tamaño del nuevo conjunto de datos es: {}'.format(len(datos_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ya tratamos los datos NaN, ahora podemos convertir las columnas a <code> int </code> a excepcion de BMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HighBP                    int32\n",
       "HighChol                  int32\n",
       "CholCheck                 int32\n",
       "Smoker                    int32\n",
       "Stroke                    int32\n",
       "HeartDiseaseorAttack      int32\n",
       "PhysActivity              int32\n",
       "Fruits                    int32\n",
       "Veggies                   int32\n",
       "HvyAlcoholConsump         int32\n",
       "AnyHealthcare             int32\n",
       "NoDocbcCost               int32\n",
       "GenHlth                   int32\n",
       "MentHlth                  int32\n",
       "PhysHlth                  int32\n",
       "DiffWalk                  int32\n",
       "Sex                       int32\n",
       "Age                       int32\n",
       "Education                 int32\n",
       "Income                    int32\n",
       "Diabetes_012              int32\n",
       "BMI                     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos_train_BMI= datos_train['BMI']\n",
    "\n",
    "datos_train = datos_train.drop(['BMI'], axis=1)\n",
    "datos_train = datos_train.astype(dtype='int32')\n",
    "datos_train = datos_train.assign(BMI = datos_train_BMI)\n",
    "datos_train.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, revisaremos cada una de las columnasa para encontrar si estas tienen errores respecto a los rangos dados en el diccionario del conjunto de datos.\n",
    "\n",
    "Empezaremos revisando las columnas que son binarias (1 o 0 ). Para esto, revisaremos el valor máximo y mínimo de cada columna, para saber cuales de ellas tienen valores fuera del rango definido en el diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " datos_train['HighBP'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1])]\n"
     ]
    }
   ],
   "source": [
    "lista_binarias=['HighBP', 'HighChol', 'CholCheck', 'Smoker', 'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'DiffWalk', 'Sex']\n",
    "lista_unicas=[]\n",
    "for i in lista_binarias:\n",
    "    lista_unicas.append(datos_train['HighBP'].unique())\n",
    "\n",
    "print(lista_unicas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con este resultado, podemos concluir que las variables que tienen posibles valores binarios, no tienen valores por fuera de su rango. \n",
    "\n",
    "Procederemos a mirar las variables no binarias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< BMI > - Max: 98.0  Min: -35.0\n",
      "< GenHlth > - Max: 5  Min: 1\n",
      "< MentHlth > - Max: 36  Min: -37\n",
      "< PhysHlth > - Max: 42  Min: -43\n",
      "< Age > - Max: 13  Min: 1\n",
      "< Education > - Max: 6  Min: 1\n",
      "< Income > - Max: 8  Min: 1\n"
     ]
    }
   ],
   "source": [
    "lista_binarias=['BMI', 'GenHlth', 'MentHlth', 'PhysHlth', 'Age', 'Education', 'Income']\n",
    "for i in lista_binarias:\n",
    "    print(\"<\",i,\"> - Max: {}\".format(datos_train[i].max()), \" Min: {}\".format(datos_train[i].min()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con lo obtenido y comparando con el diccionario que nos dan, nos damos cuenta que las variables que están fuera de los rangos establecidos son:\n",
    "* BMI (debe ser >1 y <99): Se pasa del límite inferior\n",
    "* MentHlth (debe ser >1 y <30) :Se pasa del límite inferior y superior\n",
    "* PhysHlth (debe ser >1 y <30) :Se pasa del límite inferior y superior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos fuera del rango para BMI:  209\n",
      "Datos fuera del rango para MentHlth:  242\n",
      "Datos fuera del rango para PhysHlth:  302\n",
      "En total hay 753 datos fuera del rango\n"
     ]
    }
   ],
   "source": [
    "fuera_rango_BMI=len(datos_train[datos_train[\"BMI\"]<1])\n",
    "fuera_rango_MentHlth=len(datos_train[datos_train[\"MentHlth\"]<0]) + len(datos_train[datos_train[\"MentHlth\"]>30])\n",
    "fuera_rango_PhysHlth=len(datos_train[datos_train[\"PhysHlth\"]<0]) + len(datos_train[datos_train[\"PhysHlth\"]>30])\n",
    "total= fuera_rango_BMI + fuera_rango_MentHlth + fuera_rango_PhysHlth                                                              \n",
    "print(\"Datos fuera del rango para BMI: \",str(fuera_rango_BMI))\n",
    "\n",
    "print(\"Datos fuera del rango para MentHlth: \", str(fuera_rango_MentHlth))\n",
    "\n",
    "print(\"Datos fuera del rango para PhysHlth: \", str(fuera_rango_PhysHlth))\n",
    "                                                                                 \n",
    "print(\"En total hay {} datos fuera del rango\".format(total))\n",
    "                                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay : 753 datos fuera del rango, lo cual representa el 0.95% de datos train.\n"
     ]
    }
   ],
   "source": [
    "porcentaje= total/datos_train.shape[0] *100\n",
    "print('Hay : {}'.format(total), \"datos fuera del rango, lo cual representa el {:.2f}%\".format(porcentaje), \"de datos train.\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque pueden haber filas con más de un valor fuera de rango, que el máximo valor posible de filas con valores fuera de rango es de 0.95%. Como este porcentaje es muy pequeño, procederemos a eliminar estas filas de datos train.\n",
    "\n",
    "**Nota:** En el diccionario de los datos se indica que el rango de valores válidos para las variables MentHlth y PhystHlth es >=2 y <=29. Sin embargo, teniendo en cuenta la naturaleza y/o el significado de estas dos variables, se tomó la decisión de utilizar el rango >=0 y <=30 ya que, no existe ninguna razón por la cual una persona no pueda indicar que tuvo 0, 1 o 30 días de mala salud (física o mental) en el mes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_train.drop(datos_train[datos_train[\"BMI\"]<1].index, inplace=True)\n",
    "\n",
    "datos_train.drop(datos_train[datos_train[\"MentHlth\"]<0].index, inplace=True)\n",
    "datos_train.drop(datos_train[datos_train[\"MentHlth\"]>30].index, inplace=True)\n",
    "\n",
    "datos_train.drop(datos_train[datos_train[\"PhysHlth\"]<0].index, inplace=True)\n",
    "datos_train.drop(datos_train[datos_train[\"PhysHlth\"]>30].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de eliminar estas columnas, hacemos la comprobación de que se eliminaron correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos fuera del rango para BMI:  0\n",
      "Datos fuera del rango para MentHlth:  0\n",
      "Datos fuera del rango para PhysHlth:  0\n",
      "En total hay 0 datos fuera del rango\n",
      "El tamaño del nuevo conjunto de datos es: 78393\n"
     ]
    }
   ],
   "source": [
    "fuera_rango_BMI=len(datos_train[datos_train[\"BMI\"]<1])\n",
    "fuera_rango_MentHlth=len(datos_train[datos_train[\"MentHlth\"]<0]) + len(datos_train[datos_train[\"MentHlth\"]>30])\n",
    "fuera_rango_PhysHlth=len(datos_train[datos_train[\"PhysHlth\"]<0]) + len(datos_train[datos_train[\"PhysHlth\"]>30])\n",
    "total= fuera_rango_BMI + fuera_rango_MentHlth + fuera_rango_PhysHlth                                                              \n",
    "print(\"Datos fuera del rango para BMI: \",str(fuera_rango_BMI))\n",
    "\n",
    "print(\"Datos fuera del rango para MentHlth: \", str(fuera_rango_MentHlth))\n",
    "\n",
    "print(\"Datos fuera del rango para PhysHlth: \", str(fuera_rango_PhysHlth))\n",
    "                                                                                 \n",
    "print(\"En total hay {} datos fuera del rango\".format(total))\n",
    "\n",
    "print('El tamaño del nuevo conjunto de datos es: {}'.format(len(datos_train)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#De igual manera, se revisará los outliers que hay en estas tres variables, para esto, nos apoyaremos en #los diagramas de caja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datos_train.boxplot(column=[\"BMI\", \"MentHlth\", \"MentHlth\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a revisar si hay filas duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número de filas duplicadas es 4072\n"
     ]
    }
   ],
   "source": [
    "duplicados=datos_train.duplicated()\n",
    "suma=0\n",
    "for i in duplicados:\n",
    "    if i == True:\n",
    "       # print (i)\n",
    "        suma +=1\n",
    "print (\"El número de filas duplicadas es \"+str (suma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedemos a eliminar estas filas duplicadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74321, 22)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos_train=datos_train.drop_duplicates()\n",
    "datos_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para finalizar esta sección de limpieza de datos, revisaremos si debemos hacer un balanceo entre las clases correspondientes a nuestra variable de interés.\n",
    "\n",
    "**Inserte procedimiento para hacer balanceo de datos :)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más adelante, hacemos un <code>Pandas Profiling</code>. Ahora, debido a que corregimos el asunto de los tipos de datos y las inconsistencias, sí podemos generarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "concat() got an unexpected keyword argument 'join_axes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-f1612dde2559>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprofiling\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mProfileReport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatos_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprofiling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas_profiling\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, df, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sample'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mdescription_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         self.html = to_html(sample,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas_profiling\\describe.py\u001b[0m in \u001b[0;36mdescribe\u001b[1;34m(df, bins, check_correlation, correlation_threshold, correlation_overrides, check_recoded, pool_size, **kwargs)\u001b[0m\n\u001b[0;32m    390\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m     \u001b[0mvariable_stats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mldesc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoin_axes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m     \u001b[0mvariable_stats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: concat() got an unexpected keyword argument 'join_axes'"
     ]
    }
   ],
   "source": [
    "profiling = ProfileReport(datos_train)\n",
    "profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALORES FUERA DE RANGO\n",
    "# Pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
